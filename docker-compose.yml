version: '3.9'

x-airflow-common:
    &airflow-common
    # In order to add custom dependencies or upgrade provider packages you can use your extended image.
    # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml
    # and uncomment the "build" line below, Then run `docker-compose build` to build the images.
    image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.7.0}
    # build: .
    environment:
        &airflow-common-env
        AIRFLOW__CORE__EXECUTOR: CeleryExecutor
        AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:postgres@mypostgres/postgres
        # For backward compatibility, with Airflow <2.3
        AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:postgres@mypostgres/postgres
        AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://postgres:postgres@mypostgres/postgres
        AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
        AIRFLOW__CORE__FERNET_KEY: ''
        AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
        AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
        AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
        # yamllint disable rule:line-length
        # Use simple http server on scheduler for health checks
        # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server
        # yamllint enable rule:line-length
        AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
        # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks
        # for other purpose (development, test and especially production usage) build/extend Airflow image.
        _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    volumes:
        - ./dockerfiles/airflow/config:/opt/airflow/config
        - ./dockerfiles/airflow/plugins:/opt/airflow/plugins
        - ./data/airflow:/usr/local/airflow/data
        - ./logs/airflow:/opt/airflow/logs
        - ./dockerfiles/airflow/dags:/opt/airflow/dags
        - ./datosacargar:/datosacargar
        - ./dockerfiles/airflow/requirements/requirements.txt:/requirements.txt
    user: "${AIRFLOW_UID:-50000}:0"
    depends_on:
        &airflow-common-depends-on
        redis:
            condition: service_healthy
        postgres:
            condition: service_healthy

services:
    airflow-webserver:
        <<: *airflow-common
        command: webserver
        ports:
            - "8080:8080"
        healthcheck:
            test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 30s
        restart: always
        depends_on:
            <<: *airflow-common-depends-on
            airflow-init:
                condition: service_completed_successfully
        networks:
            - tfm-network

    airflow-scheduler:
        <<: *airflow-common
        command: scheduler
        healthcheck:
            test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 30s
        restart: unless-stopped
        depends_on:
            <<: *airflow-common-depends-on
            airflow-init:
                condition: service_completed_successfully
        networks:
            - tfm-network

    airflow-worker:
        <<: *airflow-common
        command: celery worker
        healthcheck:
        # yamllint disable rule:line-length
            test:
                - "CMD-SHELL"
                - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 30s
        environment:
            <<: *airflow-common-env
            # Required to handle warm shutdown of the celery workers properly
            # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
            DUMB_INIT_SETSID: "0"
        restart: unless-stopped
        depends_on:
            <<: *airflow-common-depends-on
            airflow-init:
                condition: service_completed_successfully
        networks:
            - tfm-network

    airflow-triggerer:
        <<: *airflow-common
        command: triggerer
        healthcheck:
            test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 30s
        restart: always
        depends_on:
            <<: *airflow-common-depends-on
            airflow-init:
                condition: service_completed_successfully
        networks:
            - tfm-network

    airflow-init:
        <<: *airflow-common
        entrypoint: /bin/bash
        # yamllint disable rule:line-length
        command:
        - -c
        - |
            function ver() {
            printf "%04d%04d%04d%04d" $${1//./ }
            }
            airflow_version=$$(AIRFLOW__LOGGING__LOGGING_LEVEL=INFO && gosu airflow airflow version)
            airflow_version_comparable=$$(ver $${AIRFLOW_VERSION})
            min_airflow_version=2.2.0
            min_airflow_version_comparable=$$(ver $${min_airflow_version})
            if (( airflow_version_comparable < min_airflow_version_comparable )); then
            echo
            echo -e "\033[1;31mERROR!!!: Too old Airflow version $${airflow_version}!\e[0m"
            echo "The minimum Airflow version supported: $${min_airflow_version}. Only use this or higher!"
            echo
            exit 1
            fi
            if [[ -z "${AIRFLOW_UID}" ]]; then
            echo
            echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
            echo "If you are on Linux, you SHOULD follow the instructions below to set "
            echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
            echo "For other operating systems you can get rid of the warning with manually created .env file:"
            echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
            echo
            fi
            one_meg=1048576
            mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
            cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
            disk_available=$$(df / | tail -1 | awk '{print $$4}')
            warning_resources="false"
            if (( mem_available < 4000 )) ; then
            echo
            echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
            echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
            echo
            warning_resources="true"
            fi
            if (( cpus_available < 2 )); then
            echo
            echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
            echo "At least 2 CPUs recommended. You have $${cpus_available}"
            echo
            warning_resources="true"
            fi
            if (( disk_available < one_meg * 10 )); then
            echo
            echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
            echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
            echo
            warning_resources="true"
            fi
            if [[ $${warning_resources} == "true" ]]; then
            echo
            echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
            echo "Please follow the instructions to increase amount of resources available:"
            echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
            echo
            fi
            mkdir -p /sources/logs /sources/dags /sources/plugins
            chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
            exec /entrypoint airflow version
        # yamllint enable rule:line-length
        environment:
            <<: *airflow-common-env
            _AIRFLOW_DB_MIGRATE: 'true'
            _AIRFLOW_WWW_USER_CREATE: 'true'
            _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
            _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
            _PIP_ADDITIONAL_REQUIREMENTS: ''
            user: "0:0"
        volumes:
            - ./dockerfiles/airflow/sources:/sources
        networks:
            - tfm-network

    airflow-cli:
        <<: *airflow-common
        profiles:
            - debug
        environment:
            <<: *airflow-common-env
            CONNECTION_CHECK_MAX_COUNT: "0"
            # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
        command:
            - bash
            - -c
            - airflow
        networks:
            - tfm-network

    # You can enable flower by adding "--profile flower" option e.g. docker-compose --profile flower up
    # or by explicitly targeted on the command line e.g. docker-compose up flower.
    # See: https://docs.docker.com/compose/profiles/
    flower:
        <<: *airflow-common
        command: celery flower
        profiles:
            - flower
        ports:
            - "5555:5555"
        healthcheck:
            test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 30s
        restart: unless-stopped
        depends_on:
            <<: *airflow-common-depends-on
            airflow-init:
                condition: service_completed_successfully
        networks:
            - tfm-network

    #procesar informacio
    jupyter:
        image: jupyter/base-notebook:latest
        volumes:
            - ./data/postgres:/data/postgres
            - ./datosacargar:/datosacargar
        ports:
            - "8888:8888"
        environment:
            PGHOST: postgres
            PGUSER: postgres
            PGPASSWORD: postgres
            PGDATABASE: postgres
        networks:
            - tfm-network

    # configuration manager for NiFi
    zookeeper:
        hostname: myzookeeper
        container_name: zookeeper_container
        image: 'bitnami/zookeeper:3.7.0'  # latest image as of 2021-11-08.
        environment:
            - ALLOW_ANONYMOUS_LOGIN=yes
        networks:
            - tfm-network
        restart: always

    # data extraction, transformation and load service
    nifi:
        hostname: mynifi
        container_name: nifi_container
        image: 'apache/nifi:1.19.0'  # latest image as of 2021-11-08.
        ports:
            - '8091:8080'
        networks:
            - tfm-network
        volumes:
            - nifi-database_repository:/opt/nifi/nifi-current/database_repository
            - nifi-flowfile_repository:/opt/nifi/nifi-current/flowfile_repository
            - nifi-content_repository:/opt/nifi/nifi-current/content_repository
            - nifi-provenance_repository:/opt/nifi/nifi-current/provenance_repository
            - nifi-conf:/opt/nifi/nifi-current/conf
            - nifi-state:/opt/nifi/nifi-current/state
            - ./logs/nifi:/opt/nifi/nifi-current/logs
            - ./dockerfiles/nifi/jdbc:/opt/nifi/nifi-current/jdbc
            - ./dockerfiles/nifi/credentials:/opt/nifi/nifi-current/credentials
        environment:
            - NIFI_WEB_HTTP_PORT=8080
            - NIFI_CLUSTER_IS_NODE=true
            - NIFI_CLUSTER_NODE_PROTOCOL_PORT=8082
            - NIFI_ZK_CONNECT_STRING=myzookeeper:2181
            - NIFI_ELECTION_MAX_WAIT=30 sec
            - NIFI_SENSITIVE_PROPS_KEY='12345678901234567890A'
        restart: on-failure
        healthcheck:
            test: ["CMD", "curl", "-f", "http://mynifi:8080/nifi/"]
            interval: 30s
            timeout: 20s
            retries: 3

    # version control for nifi flows
    registry:
        hostname: myregistry
        container_name: registry_container
        image: 'apache/nifi-registry:1.23.0'
        restart: on-failure
        ports:
            - "18080:18080"
        environment:
            - LOG_LEVEL=INFO
            - NIFI_REGISTRY_DB_DIR=/opt/nifi-registry/nifi-registry-current/database
            - NIFI_REGISTRY_FLOW_PROVIDER=file
            - NIFI_REGISTRY_FLOW_STORAGE_DIR=/opt/nifi-registry/nifi-registry-current/flow_storage
        volumes:
            - ./data/nifiregistry/database:/opt/nifi-registry/nifi-registry-current/database
            - ./data/nifiregistry/flow_storage:/opt/nifi-registry/nifi-registry-current/flow_storage
            - ./logs/nifiregistry:/opt/nifi-registry/nifi-registry-current/logs
        networks:
            - tfm-network
        healthcheck:
            test: ["CMD", "curl", "-f", "http://myregistry:18080/nifi-registry/"]
            interval: 30s
            timeout: 20s
            retries: 3

    superset:
        image: tylerfowler/superset
        container_name: superset
        restart: always
        environment:
            MAPBOX_API_KEY: ${MAPBOX_API_KEY}
        ports:
            - "8088:8088"
        networks:
            - tfm-network

    # relational database
    postgres:
        hostname: mypostgres
        container_name: postgres_container
        image: 'postgres:14-bullseye'  # latest image as of 2021-11-08
        environment:
            POSTGRES_USER: 'postgres'
            POSTGRES_PASSWORD: 'postgres'
            PGDATA: /data/postgres
        volumes:
            - ./data/postgres:/data/postgres
            - ./datosacargar:/datosacargar
        ports:
            - "5432:5432"
        networks:
            - tfm-network
        restart: on-failure
        healthcheck:
            test: ["CMD", "pg_isready"]
            interval: 30s
            timeout: 20s
            retries: 3
    # database administration tool
    pgadmin:
        hostname: mypgadmin
        container_name: pgadmin_container
        image: 'dpage/pgadmin4:7.6'
        environment:
            PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL}
            PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_DEFAULT_PASSWORD}
            PGADMIN_CONFIG_SERVER_MODE: ${PGADMIN_CONFIG_SERVER_MODE}
        volumes:
            - ./data/pgadmin:/var/lib/pgadmin
        ports:
            - "5050:80"
        networks:
            - tfm-network
        restart: on-failure
        healthcheck:
            test: ["CMD", "curl", "-f", "http://mypgadmin:80/misc/ping"]
            interval: 30s
            timeout: 20s
            retries: 3

    # object storage
    minio:
        hostname: myminio
        container_name: minio_container
        image: 'bitnami/minio:2023'
        environment:
            MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY}
            MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
        ports:
            - '9000:9000'
            - '9001:9001'
        volumes:
            - './data/minio:/data'
        networks:
            - tfm-network
        healthcheck:
            test: ["CMD", "curl", "-f", "http://myminio:9000/minio/health/live"]
            interval: 30s
            timeout: 20s
            retries: 3
    redis:
        image: redis:latest
        expose:
        - 6379
        healthcheck:
            test: ["CMD", "redis-cli", "ping"]
            interval: 10s
            timeout: 30s
            retries: 50
            start_period: 30s
        restart: unless-stopped
        networks:
            - tfm-network

networks:
  tfm-network:
    name: tfm-network

volumes:
    airflow-data:
    nifi-database_repository:
    nifi-flowfile_repository:
    nifi-content_repository:
    nifi-provenance_repository:
    nifi-state:
    nifi-conf:
